# -*- coding: utf-8 -*-
"""House Price Prediction (21016).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18oqaaMrrZtafspzSydOekTyWEcHG2VHf

Â© Ritwik Chandra Pandey


2nd MSc (Maths) - Specialisation in CS

##IMPORTING LIBRARIES
"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.tree import DecisionTreeRegressor
import seaborn as sns
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import export_graphviz
import pydot
from IPython.display import Image, display
import warnings

"""## IGNORING WARNINGS"""

warnings.filterwarnings('ignore')

"""##IMPORTING THE DATASET"""

Train_data_full = pd.read_csv('train.csv')
Test_data_full = pd.read_csv('test.csv')

"""## BASIC INFORMATION ABOUT THE DATASET

### Number of Null values in each column of Train_data_full and Test_data_full
"""

pd.set_option('display.max_rows', None)
Train_data_full.isnull().sum()

Test_data_full.isnull().sum()

Train_data_full.info()

Test_data_full.info()

#Describing the label to be predicted in train.scv
Train_data_full['SalePrice'].describe()

"""##DATA PREPROCESSING

### DROP ID COLUMN
"""

Train_data = Train_data_full.drop(['Id'], axis=1)
Test_data = Test_data_full.drop(['Id'], axis=1)

"""### CHECKING FOR SKEWNESS OF SALE PRICE"""

fig = plt.figure(figsize=(18,8))
sns.distplot(Train_data['SalePrice'])
plt.suptitle( "Plot of Sale Price")
print("Skewness: %f" % Train_data['SalePrice'].skew() + "\n")
plt.show()

"""We shall apply log transformation to correct the positive skewness in the data

More importantly, taking logs means that errors in predicting expensive and cheap houses will affect the result equally.

"""

fig = plt.figure(figsize=(18,8))
Train_data['SalePrice'] = np.log1p(Train_data['SalePrice'])
plt.suptitle("Plot of Sale Price after log transformation")
sns.distplot(Train_data['SalePrice'])
plt.show()

"""### HANDLING NULL VALUES"""

def missing (df):
    
    # drop theses columns due to large numbeer of null values or many same values
    df = df.drop(['Utilities','PoolQC','MiscFeature','Alley'], axis=1) 
    # Null value likely means No Fence so fill as "None"
    df["Fence"] = df["Fence"].fillna("None")
    # Null value likely means No Fireplace so fill as "None"
    df["FireplaceQu"] = df["FireplaceQu"].fillna("None")
    # Lot frontage is the feet of street connected to property, which is likely similar to the neighbourhood houses, so fill Median value
    df["LotFrontage"] = df["LotFrontage"].fillna(df["LotFrontage"].median())
    # Null value likely means typical(Typ)
    df["Functional"] = df["Functional"].fillna("Typ")
    # Only one null value so fill as the most frequent value(mode)
    df['KitchenQual'] = df['KitchenQual'].fillna(df['KitchenQual'].mode()[0])
    # Only one null value so fill as the most frequent value(mode)
    df['Electrical'] = df['Electrical'].fillna(df['Electrical'].mode()[0])
    # Very few null value so fill with the most frequent value(mode)
    df['SaleType'] = df['SaleType'].fillna(df['SaleType'].mode()[0])
    # Null value likely means no masonry veneer
    df["MasVnrType"] = df["MasVnrType"].fillna("None") #so fill as "None" (since categorical feature)
    df["MasVnrArea"] = df["MasVnrArea"].fillna(0)
    # Only one null value so fill as the most frequent
    df['Exterior1st'] = df['Exterior1st'].fillna(df['Exterior1st'].mode()[0])
    df['Exterior2nd'] = df['Exterior2nd'].fillna(df['Exterior2nd'].mode()[0])
    #MSZoning is general zoning classification,Very few null value so fill with the most frequent value(mode)
    df['MSZoning'] = df['MSZoning'].fillna(df['MSZoning'].mode()[0])
    #Null value likely means no Identified type of dwelling so fill as "None"
    df['MSSubClass'] = df['MSSubClass'].fillna("None")
    
    # Null value likely means No Garage, so fill as "None" (since these are categorical features)
    for col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):
        df[col] = df[col].fillna('None')
    # Null value likely means No Garage and no cars in garage, so fill as 0
    for col in ('GarageYrBlt', 'GarageArea', 'GarageCars'): 
        df[col] = df[col].fillna(0)
    
    # Null value likely means No Basement, so fill as 0
    for col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):
        df[col] = df[col].fillna(0)
    
    # Null value likely means No Basement, so fill as "None" (since these are categorical features)
    for col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):
        df[col] = df[col].fillna('None')
    return df

Train_ = missing(Train_data)
Test_ = missing(Test_data)
# calculate total number of null values in training data
null_train_set = Train_.isnull().sum().sum()
print(null_train_set)
# calculate total number of null values in test data
null_test_set = Test_.isnull().sum().sum()
print(null_test_set)

"""### MERGING RELATED COLUMNS BY ADDING"""

def add_new_cols(df):
    df['Total_SF'] = df['TotalBsmtSF'] + df['1stFlrSF'] + df['2ndFlrSF']
    df['Total_Bathrooms'] = (df['FullBath'] + (0.5 * df['HalfBath']) + \
                             df['BsmtFullBath'] + (0.5 * df['BsmtHalfBath']))
    df['Total_Porch_SF'] = (df['OpenPorchSF'] + df['3SsnPorch'] + df['EnclosedPorch']) 
    df['Total_Square_Feet'] = (df['BsmtFinSF1'] + df['BsmtFinSF2'] + df['1stFlrSF'] + df['2ndFlrSF'])
    df['Total_Quality'] = df['OverallQual'] + df['OverallCond']
    
    return df

Train_added = add_new_cols(Train_)
Test_added = add_new_cols(Test_)

"""### GETTING DUMMY VALUES FOR CATEGORICAL DATA"""

columnsToEncode = Train_added.select_dtypes(include=[object]).columns
columnsToEncode1 = Test_added.select_dtypes(include=[object]).columns
Train_encoded = pd.get_dummies(Train_added, columns=columnsToEncode, drop_first=False)
Test_encoded = pd.get_dummies(Test_added, columns=columnsToEncode, drop_first=False)
Train_y = Train_encoded['SalePrice']
Train_encoded = Train_encoded.drop(['SalePrice'], axis = 1)

"""###ALIGNING TRAINING AND TESTING DATA USING INNER JOIN"""

#List of all columns in Train_encoded which is not part of Test_encoded
list(set(list(Train_encoded.columns)) - set(list(Test_encoded.columns)))

#Inner join to remove columns in Train_encoded which are not part of Test_encoded
Train_aligned, Test_aligned= Train_encoded.align(Test_encoded, join = 'inner', axis=1) 
print(Train_aligned.shape)
print(Test_aligned.shape)

"""## SPLITTING THE DATASET"""

X, y = Train_aligned, Train_y
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=118)

"""#MULTIPLE LINEAR REGRESSION

###Simple Linear Regression is not possible in this case as there are multiple independent variables.
"""

regressor = LinearRegression()
regressor.fit(X_train, y_train)
y_pred = regressor.predict(X_test)
y_train_pred = regressor.predict(X_train)
train_mse = mean_squared_error(y_train_pred, y_train)
test_mse = mean_squared_error(y_test, y_pred)
print('Train MSE: ' + str(train_mse) + '\n')
print('Test MSE: ' + str(test_mse) + '\n')
y_test_pred_mlr = np.expm1(regressor.predict(Test_aligned))

"""#POLYNOMIAL REGRESSION"""

poly_reg = PolynomialFeatures(degree = 2)
X_poly = poly_reg.fit_transform(X_train)
X_poly_test = poly_reg.fit_transform(X_test)
lin_reg = LinearRegression()
lin_reg.fit(X_poly, y_train)
y_train_pred = lin_reg.predict(X_poly)
y_pred = lin_reg.predict(X_poly_test)
train_mse = mean_squared_error(y_train_pred, y_train)
test_mse = mean_squared_error(y_test, y_pred)
print('Train MSE: ' + str(train_mse) + '\n')
print('Test MSE: ' + str(test_mse) + '\n')
y_test_pred_pr = np.expm1(lin_reg.predict(poly_reg.fit_transform(Test_aligned)))

"""##REGRESSION USING DECISION TREE"""

regressorDT = DecisionTreeRegressor(random_state = 0)
regressorDT.fit(X_train, y_train)
y_pred = regressorDT.predict(X_test)
y_train_pred = regressorDT.predict(X_train)
train_mse = mean_squared_error(y_train_pred, y_train)
test_mse = mean_squared_error(y_test, y_pred)
print('Train MSE: ' + str(train_mse) + '\n')
print('Test MSE: ' + str(test_mse) + '\n')
y_test_pred_dt = np.expm1(regressorDT.predict(Test_aligned))

"""##REGRESSION USING RANDOM FOREST"""

rf = RandomForestRegressor(n_estimators = 1000, random_state = 42)
rf.fit(X_train, y_train)
y_pred = rf.predict(X_test)
y_train_pred = rf.predict(X_train)
y_test_pred_rf = np.expm1(rf.predict(Test_aligned))
train_mse1 = mean_squared_error(y_train_pred, y_train)
test_mse1 = mean_squared_error(y_test, y_pred)
print('Train MSE: ' + str(train_mse1) + '\n')
print('Test MSE: ' + str(test_mse1) + '\n')

"""### Finding importance of each feature and visualising them"""

importances = list(rf.feature_importances_)
feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(list(X.columns), importances)]
feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)
[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances];
most_importance = [x for x in feature_importances if x[1] != 0.0] 
most_importance_names = [x[0] for x in most_importance]
most_importance_values = [x[1] for x in most_importance]

"""###Plotting only for those features that have importance more than 0.0"""

plt.figure(figsize=(20,15))
x_values = list(range(len(most_importance_values)))
plt.bar(x_values, most_importance_values, orientation = 'vertical')

plt.xticks(x_values, most_importance_names, rotation='vertical')

plt.ylabel('Importance'); plt.xlabel('Variable'); plt.title('Variable Importances');

"""### Visualising Random Forest Graph"""

tree = rf.estimators_[5]
export_graphviz(tree, out_file = 'tree.dot', feature_names = list(X.columns), rounded = True, precision = 1)
(graph, ) = pydot.graph_from_dot_file('tree.dot')
graph.write_png('tree.png')
display(Image('tree.png'))

"""### Building a new model by only taking the most important metrics"""

rf_most_important = RandomForestRegressor(n_estimators= 1000, random_state=42)
important_indices = [ 'OverallQual', 'Total_SF', 'Total_Square_Feet']
train_important = X_train[important_indices].copy()
test_important = X_test[important_indices].copy()
rf_most_important.fit(train_important, y_train)
predictions = rf_most_important.predict(test_important)
predictions_train = rf_most_important.predict(train_important)
predictions_test_pred_rfi = np.expm1(rf_most_important.predict(Test_aligned[[ 'OverallQual', 'Total_SF', 'Total_Square_Feet']]))
train_mse2 = mean_squared_error(predictions_train, y_train)
test_mse2 = mean_squared_error(y_test, predictions )
print('Train MSE: ' + str(train_mse2) + '\n')
print('Test MSE: ' + str(test_mse2) + '\n')

"""### Comparing both Random Forest models"""

print('Train MSE for Random Forest Model: ' + str(train_mse1) )
print('Test MSE for Random Forest Model: ' + str(test_mse1) + '\n')
print('Train MSE for Random Forest Model with only important features: ' + str(train_mse2))
print('Test MSE for Random Forest Model with only important features: ' + str(test_mse2) + '\n')

"""## Comparing predictions on Test Set of all models"""

df_mlr = pd.DataFrame(y_test_pred_mlr, columns = ['MLR'])
df_pr = pd.DataFrame(y_test_pred_pr, columns = ['PR'])
df_dt = pd.DataFrame(y_test_pred_dt, columns = ['DT'])
df_rf = pd.DataFrame(y_test_pred_rf, columns = ['RF'])
df_rfi = pd.DataFrame(predictions_test_pred_rfi, columns = ['RF (Imp)'])
df_all = pd.concat([df_mlr, df_pr, df_dt, df_rf, df_rfi], axis = 1)

df_all

