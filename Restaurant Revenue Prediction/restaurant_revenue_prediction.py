# -*- coding: utf-8 -*-
"""Restaurant_Revenue_Prediction_(21016).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hyZuFiM_NiskbRYdxiPhkhz0Fvgv6Bu3

Â© Ritwik Chandra Pandey


2nd MSc (Maths) - Specialisation in CS

##IMPORTING LIBRARIES
"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.tree import DecisionTreeRegressor
from sklearn.preprocessing import OneHotEncoder
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
import datetime
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import export_graphviz
import pydot
from IPython.display import Image, display
import warnings
import seaborn as sns

"""## IGNORING WARNINGS"""

warnings.filterwarnings('ignore')

"""##IMPORTING THE DATASET"""

Train_data = pd.read_csv('train.csv')
Test_data = pd.read_csv('test.csv')

"""## BASIC INFORMATION ABOUT THE DATASET

### Number of Null values in each column of Train_data and Test_data
"""

pd.set_option('display.max_rows', None)
Train_data.isnull().sum()

Test_data.isnull().sum()

Train_data.info()

Test_data.info()

#Describing the label to be predicted in train.scv
Train_data['revenue'].describe()

"""##DATA PREPROCESSING

The zero values in P columns are actually null values. So, Multivariate imputation by chained equations is used to replace the missing values in some of these features.
"""

Train_data = Train_data.dropna() #Dropping rows with NaN values
imp_train = IterativeImputer(max_iter=5, missing_values=0, sample_posterior=True, min_value=1, random_state=37)
imp_test = IterativeImputer(max_iter=5, missing_values=0, sample_posterior=True, min_value=1, random_state=23)
p_data = ['P'+str(i) for i in range(1,38)]
Train_data[p_data] = np.round(imp_train.fit_transform(Train_data[p_data]))
Test_data[p_data] = np.round(imp_test.fit_transform(Test_data[p_data]))

"""Using encoder to encode some columns as part of preprocessing."""

Encoder =  OneHotEncoder()

"""Preprocessing data in Train.csv file. 


"""

Train_data_dropped = Train_data.drop(['Id', 'City'], axis=1)
city_reshaped = np.array(Train_data_dropped['City Group']).reshape(-1, 1)
city_values = Encoder.fit_transform(city_reshaped)
city_type = pd.DataFrame(city_values.toarray(), columns=['Big Cities', 'Other'])
type_reshaped =np.array(Train_data_dropped['Type']).reshape(-1, 1)
type_values = Encoder.fit_transform(type_reshaped)
type_encoded = pd.DataFrame(type_values.toarray(), columns=['DT', 'FC', 'IL'])
df_encoded = pd.concat([city_type, type_encoded], axis=1)
Train_encoded = pd.concat([df_encoded, Train_data_dropped], axis=1)
Train_encoded = Train_encoded.drop(['City Group', 'Type'], axis=1)

"""Preprocessing data in Test.csv file."""

Test_data_dropped = Test_data.drop(['Id', 'City'], axis=1)
#In train data there is no Type MB so we replace MB with DT in test data (only one is there).
Test_data_dropped.loc[Test_data_dropped['Type'] == 'MB', 'Type'] = 'DT' 
city_reshaped = np.array(Test_data_dropped['City Group']).reshape(-1, 1)
city_values = Encoder.fit_transform(city_reshaped)
city_type = pd.DataFrame(city_values.toarray(), columns=['Big Cities', 'Other'])
type_reshaped =np.array(Test_data_dropped['Type']).reshape(-1, 1)
type_values = Encoder.fit_transform(type_reshaped)
type_encoded = pd.DataFrame(type_values.toarray(), columns=['DT', 'FC', 'IL'])
df_encoded = pd.concat([city_type, type_encoded], axis=1)
Test_encoded = pd.concat([df_encoded, Test_data_dropped], axis=1)
Test_encoded = Test_encoded.drop(['City Group', 'Type'], axis=1)

"""Handling Date column so that number of days the restruant was open can be taken into consideration. """

Train_encoded['Open Date']  = pd.to_datetime(Train_encoded['Open Date'])
Test_encoded['Open Date']  = pd.to_datetime(Test_encoded['Open Date'])
Launch_date_max = max(max(Test_encoded['Open Date']), max(Train_encoded['Open Date']))
Train_encoded['Days Open'] = (Launch_date_max - Train_encoded['Open Date']).dt.days / 1000
Test_encoded['Days Open'] = (Launch_date_max - Test_encoded['Open Date']).dt.days / 1000
Train_encoded.drop('Open Date', axis=1, inplace=True)
Test_encoded.drop('Open Date', axis=1, inplace=True)

"""### CHECKING FOR SKEWNESS OF REVENUE"""

fig = plt.figure(figsize=(18,8))
sns.distplot(Train_encoded['revenue'])
plt.suptitle( "Plot of Revenue")
print("Skewness: %f" % Train_encoded['revenue'].skew() + "\n")
plt.show()

"""Since we will be experimenting with linear models, the target variable is transformed to make it normally distributed for improved model interpretation. Definitely, the final predictions will need to be exponentiated to rescale the results back to normal."""

fig = plt.figure(figsize=(18,8))
Train_encoded['revenue'] = np.log1p(Train_encoded['revenue'])
plt.suptitle("Plot of Revenue after log transformation")
sns.distplot(Train_encoded['revenue'])
plt.show()

"""## SPLITTING THE DATASET"""

X, y = Train_encoded.drop('revenue', axis=1), Train_encoded['revenue']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=118)

"""#MULTIPLE LINEAR REGRESSION

###Simple Linear Regression is not possible in this case as there are multiple independent variables.
"""

regressor = LinearRegression()
regressor.fit(X_train, y_train)
y_pred = regressor.predict(X_test)
y_train_pred = regressor.predict(X_train)
train_mse = mean_squared_error(y_train_pred, y_train)
test_mse = mean_squared_error(y_test, y_pred)
print('Train MSE: ' + str(train_mse) + '\n')
print('Test MSE: ' + str(test_mse) + '\n')
y_test_pred_mlr = np.expm1(regressor.predict(Test_encoded))

"""#POLYNOMIAL REGRESSION"""

poly_reg = PolynomialFeatures(degree = 2)
X_poly = poly_reg.fit_transform(X_train)
X_poly_test = poly_reg.fit_transform(X_test)
lin_reg = LinearRegression()
lin_reg.fit(X_poly, y_train)
y_train_pred = lin_reg.predict(X_poly)
y_pred = lin_reg.predict(X_poly_test)
train_mse = mean_squared_error(y_train_pred, y_train)
test_mse = mean_squared_error(y_test, y_pred)
print('Train MSE: ' + str(train_mse) + '\n')
print('Test MSE: ' + str(test_mse) + '\n')
y_test_pred_pr = np.expm1(lin_reg.predict(poly_reg.fit_transform(Test_encoded)))

"""##REGRESSION USING DECISION TREE"""

regressorDT = DecisionTreeRegressor(random_state = 0)
regressorDT.fit(X_train, y_train)
y_pred = regressorDT.predict(X_test)
y_train_pred = regressorDT.predict(X_train)
train_mse = mean_squared_error(y_train_pred, y_train)
test_mse = mean_squared_error(y_test, y_pred)
print('Train MSE: ' + str(train_mse) + '\n')
print('Test MSE: ' + str(test_mse) + '\n')
y_test_pred_dt = np.expm1(regressorDT.predict(Test_encoded))

"""##REGRESSION USING RANDOM FOREST"""

rf = RandomForestRegressor(n_estimators = 1000, random_state = 42)
rf.fit(X_train, y_train)
y_pred = rf.predict(X_test)
y_train_pred = rf.predict(X_train)
y_test_pred_rf = np.expm1(rf.predict(Test_encoded))
train_mse1 = mean_squared_error(y_train_pred, y_train)
test_mse1 = mean_squared_error(y_test, y_pred)
print('Train MSE: ' + str(train_mse1) + '\n')
print('Test MSE: ' + str(test_mse1) + '\n')

"""### Finding importance of each feature and visualising them"""

importances = list(rf.feature_importances_)
feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(list(X.columns), importances)]
feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)
[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances];

plt.figure(figsize=(25,15))
x_values = list(range(len(importances)))
plt.bar(x_values, importances, orientation = 'vertical')

plt.xticks(x_values, list(X.columns), rotation='vertical')

plt.ylabel('Importance'); plt.xlabel('Variable'); plt.title('Variable Importances');

"""### Visualising Random Forest Graph"""

tree = rf.estimators_[5]
export_graphviz(tree, out_file = 'tree.dot', feature_names = list(X.columns), rounded = True, precision = 1)
(graph, ) = pydot.graph_from_dot_file('tree.dot')
graph.write_png('tree.png')
display(Image('tree.png'))

"""### Building a new model by only taking the most important metrics"""

rf_most_important = RandomForestRegressor(n_estimators= 1000, random_state=42)
important_indices = ['P28','P34','Days Open', 'P16']
train_important = X_train[important_indices].copy()
test_important = X_test[important_indices].copy()
rf_most_important.fit(train_important, y_train)
predictions = rf_most_important.predict(test_important)
predictions_train = rf_most_important.predict(train_important)
predictions_test_pred_rfi = np.expm1(rf_most_important.predict(Test_encoded[['P28','P34','Days Open', 'P16']]))
train_mse2 = mean_squared_error(predictions_train, y_train)
test_mse2 = mean_squared_error(y_test, predictions )
print('Train MSE: ' + str(train_mse2) + '\n')
print('Test MSE: ' + str(test_mse2) + '\n')

"""### Comparing both Random Forest models"""

print('Train MSE for Random Forest Model: ' + str(train_mse1) )
print('Test MSE for Random Forest Model: ' + str(test_mse1) + '\n')
print('Train MSE for Random Forest Model with only important features: ' + str(train_mse2))
print('Test MSE for Random Forest Model with only important features: ' + str(test_mse2) + '\n')

"""## Comparing predictions on Test Set of all models"""

df_mlr = pd.DataFrame(y_test_pred_mlr, columns = ['MLR'])
df_pr = pd.DataFrame(y_test_pred_pr, columns = ['PR'])
df_dt = pd.DataFrame(y_test_pred_dt, columns = ['DT'])
df_rf = pd.DataFrame(y_test_pred_rf, columns = ['RF'])
df_rfi = pd.DataFrame(predictions_test_pred_rfi, columns = ['RF (Imp)'])
df_all = pd.concat([df_mlr, df_pr, df_dt, df_rf, df_rfi], axis = 1)

pd.set_option('display.max_rows', 10)
df_all